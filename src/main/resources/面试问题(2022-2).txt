################################################ 一 Kafka (精通) ################################################

1. Kafka 批量发送消息是如何存储的？批量消息是否需要批量消费? 压缩消息呢？
Kafka 批量发送或者压缩发送的消息都是按照发送过来的形式进行存储，并不做数据的解批或者解压缩处理.
由于存储就是按照批量进行的，消费者拉取到该批次消息进行消费时，会先将整批消息拉取下来存在消费者客户端
对应的容器中，然后根据消费者实际的消费方式(单条或者批量消费)进行ConsumerRecord的传递。在位移提交时
也是按照实际消费的数据进行提交，并不以存储的颗粒度进行提交。
假设topic-1 上面 1001 - 1100 这一百条消息是按照批进行存储的，那么实际拉取下来就是100条消息存在客户端缓存中，
若消费者是单条的方式进行消费，那么它每消费一条就进行一次offset 提交，而不是消费完所有100条再一次性进行offset提交。

压缩消息相对批量消息多了一个解压缩的过程，其它情况类似。


2. Kafka 有哪些情况会导致消息丢失?  哪些情况下消息会重复? 如何避免?

消息丢失:
①：消息生产者配置的acks = 0 或者 1,然后当某个分区leader节点故障后，如果此时消息还未被消费，并且follower也
	未同步到该消息（ISR中只有leader,没有其它follower）,新选举出来的follower就会因为从更小的offset开始记录，
	这时就导致了消息的丢失
②: 分区副本数为1, kafka默认日志刷盘是考操作系统取完成的，这样若某个分区的leader节点故障，同时消息未被刷盘的话，
	消息就会因为掉电而丢失
③: 消费者在消费消息时选择的自动提交，而不是手动提交。这样消费逻辑的执行出现异常，就会出现消费位移已经提交，但是部分逻辑执行失败
	这样对于失败的那部分逻辑而言消息是丢失了的。其实只要把消费接口做成幂等的，并且使用手提位移提交的方式就可以避免消费端的消息丢失。
以上消息丢失只需要对响应的配置做出修改即可


消息重复:
①: 生产者重复发送了消息，对于kafka而言这些消息是不一样的，但是业务角度来看都是相同的数据。
②: 消费者进行批量消费，消费到一半时出现异常，选择的手动提交消费位移，当消费者重新批量消费时已消费过的部分属于重复消费。
消息重复的情况只需要做好消费接口的幂等性设计即可。



3. Kafka 如何实现消息的顺序消费?

kafka 分区间不保证消息的顺序性，若需要对多分区进行消费同时还需要保证消息的顺序，是需要将消息拉取
下来做排序处理，之后再执行消费动作。该做法一方面效率较低，实现复杂，还面临消费位移提交的麻烦。
若条件允许可以通过以下两种消费方式实现顺序消费:
① 需要顺序消费的消息由单个生产者发送:
	单生产者发送的所有消息，在发送时就是按照顺序进行的，这时broker端指定分区leader在存储消息时也是顺序存储
    这时对应的消费者在消费时也是顺序的。这就是顺序消费了，这里关键的环节就是让这个生产者发送的消息都存储到
    一个分区中，通过相同的key 就能做到这种效果。

② 需要顺序消费的消息由多个生产者发送:
	多个生产者发送指定相同的key也可以将消息发送到同一个分区，但是多个生产者发送时消息的发送顺序和消息的存储顺序
	并不能做到一致，也就是说即使消息存储在了同一个分区，这里面的消息也是不保证有序的。
	这时就需要确保业务上的有序，可以考在消息体内增加序列号字段，选择足够大的消费批次，然后对不同key分区，对每组消息
	检查序列号是否完备，完备的进行消费，并计算出可提交的最大位移(没有间断)进行提交，同时做好消息的去重处理。




4. Kafka 消费端位移提交是如何存储的？具体工作机制是？
Kafka 消费位移提交到服务端存储在 __consumer_offset 的内部topic中, 位移以<GroupId, Topic, PartitionId> 作为key
以实际消费位移大小作为value(还包括其它信息)，存储在 __consumer_offset topic中的某一个分区中。当消费组内消费者出现
上线或者下线或者消费者重启时会从该内部topic下通过 key 获取 GroupId, Topic, PartitionId 指定的位移地址，然后再从log中找出该消息进行消费。



5. Kafka 消息的存储格式?
kafka消息存储分V0,V1和V2版本 三个不同的日志存储格式，Kafka版本对应日志格式版本的关系如下:
Kafka 0.10 以前: V0 版本日志
Kafka 0.10-0.11: V1 版本日志
Kafka 0.11 以后: V2 版本日志

kafka 消息按照 topic 进行划分，每个topic 下以分区进行细分，每个分区独立保存各自的数据(使用独立的offset)
分区为了保证高可用一般都会创建多个副本，多个副本按照一个leader 多个follower进行分工，leader直接对接生产者和
消费者，follower从leader同步数据做好数据的备份。 不论leader还是follower 都会将数据持久化到磁盘log文件，
实际log 文件在物理形式上一个文件夹，下面分多个segment,每个segment主要包含存储消息的.log 文件，.index稀疏索引
文件 和 .timeindex 时间索引文件。 这些文件以当前分区消息存储的offset 作为文件名。
其中log文件中存储的完整消息在 v2 版本中是以 batch 形式存储的。这里不展开具体格式，只做大致描述
(1) 生产者发送的ProducerBatch 对应在broker端为 RecordBatch ,批量发送过来的消息存储上也是批量存储
(2) 实际存储时每批消息数据在拉取时会被一次性拉取给到消费者,由消费者客户端进行批次内操作
(3) 压缩消息也是直接存储压缩后的消息,实际消息的解压缩处理交由消费者客户端完成(待确认...)



6. Kafka集群中Controller负责哪些工作?

7. Kafka 服务端幂等和事务指的是什么？如何使用?

8. Kafka 生产者客户端消息的发送流程?




################################################ 二 MySQL (熟练掌握) ###########################################
1. 聊一聊 MySQL的索引？
什么是索引? 索引的数据结构是怎样的? 为什么这样设计索引? 索引有哪些类型? 怎么应用索引?


2. 说一说MySQL的事务？
什么是事务? 有哪些特性? 特性是怎么实现的？事务如何做到隔离? RR隔离级别是如何实现的？


3. MySQL中有哪些锁?他们有什么用途?


4. MySQL中有哪些日志? 它们分别有什么用途?

5. 比较一下存储引擎 InnoDB 和 MyISAM？



############################################# 三 Redis (熟练原理&灵活应用) #########################################
1. Redis 有哪些数据结构? 它们底层是如何实现的？(Hash,Zset)

2. Redis 是单线程的为什么还这么快?

3. Redis 如何保证数据不丢失? 若是集群模式下如何保证?
I、非高可用情况下保证: 数据持久化(牺牲可用性保证可靠性)
Redis数据持久化有哪些方案? 两种持久化方案的工作机制? 数据如何刷盘?

II、高可用情况下保证: 集群节点故障后如何处理保证数据不丢失


4. 介绍一下Redis 集群?
Redis有哪些工作模式? 集群模式与主从模式的关系? 集群槽位划分(节点上下线)? 主从之间数据同步方式?





################################################ 四 分布式组件 (事务和锁) ###########################################


################################################ 五 JDK (并发&JVM&集合) ###########################################


################################################ 六 系统设计(项目&临时设计) #########################################


############################################## 七 其它 (网络,系统,Maven,Git) ######################################