################################################ 一 Kafka (精通) ################################################

1. Kafka 批量发送消息是如何存储的？批量消息是否需要批量消费? 压缩消息呢？
Kafka 批量发送或者压缩发送的消息都是按照发送过来的形式进行存储，并不做数据的解批或者解压缩处理.
由于存储就是按照批量进行的，消费者拉取到该批次消息进行消费时，会先将整批消息拉取下来存在消费者客户端
对应的容器中，然后根据消费者实际的消费方式(单条或者批量消费)进行ConsumerRecord的传递。在位移提交时
也是按照实际消费的数据进行提交，并不以存储的颗粒度进行提交。
假设topic-1 上面 1001 - 1100 这一百条消息是按照批进行存储的，那么实际拉取下来就是100条消息存在客户端缓存中，
若消费者是单条的方式进行消费，那么它每消费一条就进行一次offset 提交，而不是消费完所有100条再一次性进行offset提交。

压缩消息相对批量消息多了一个解压缩的过程，其它情况类似。


2. Kafka 有哪些情况会导致消息丢失?  哪些情况下消息会重复? 如何避免?

消息丢失:
①：消息生产者配置的acks = 0 或者 1,然后当某个分区leader节点故障后，如果此时消息还未被消费，并且follower也
	未同步到该消息（ISR中只有leader,没有其它follower）,新选举出来的follower就会因为从更小的offset开始记录，
	这时就导致了消息的丢失
②: 分区副本数为1, kafka默认日志刷盘是考操作系统取完成的，这样若某个分区的leader节点故障，同时消息未被刷盘的话，
	消息就会因为掉电而丢失
③: 消费者在消费消息时选择的自动提交，而不是手动提交。这样消费逻辑的执行出现异常，就会出现消费位移已经提交，但是部分逻辑执行失败
	这样对于失败的那部分逻辑而言消息是丢失了的。其实只要把消费接口做成幂等的，并且使用手提位移提交的方式就可以避免消费端的消息丢失。
以上消息丢失只需要对响应的配置做出修改即可


消息重复:
①: 生产者重复发送了消息，对于kafka而言这些消息是不一样的，但是业务角度来看都是相同的数据。
②: 消费者进行批量消费，消费到一半时出现异常，选择的手动提交消费位移，当消费者重新批量消费时已消费过的部分属于重复消费。
消息重复的情况只需要做好消费接口的幂等性设计即可。



3. Kafka 如何实现消息的顺序消费?

kafka 分区间不保证消息的顺序性，若需要对多分区进行消费同时还需要保证消息的顺序，是需要将消息拉取
下来做排序处理，之后再执行消费动作。该做法一方面效率较低，实现复杂，还面临消费位移提交的麻烦。
若条件允许可以通过以下两种消费方式实现顺序消费:
① 需要顺序消费的消息由单个生产者发送:
	单生产者发送的所有消息，在发送时就是按照顺序进行的，这时broker端指定分区leader在存储消息时也是顺序存储
    这时对应的消费者在消费时也是顺序的。这就是顺序消费了，这里关键的环节就是让这个生产者发送的消息都存储到
    一个分区中，通过相同的key 就能做到这种效果。

② 需要顺序消费的消息由多个生产者发送:
	多个生产者发送指定相同的key也可以将消息发送到同一个分区，但是多个生产者发送时消息的发送顺序和消息的存储顺序
	并不能做到一致，也就是说即使消息存储在了同一个分区，这里面的消息也是不保证有序的。
	这时就需要确保业务上的有序，可以考在消息体内增加序列号字段，选择足够大的消费批次，然后对不同key分区，对每组消息
	检查序列号是否完备，完备的进行消费，并计算出可提交的最大位移(没有间断)进行提交，同时做好消息的去重处理。




4. Kafka 消费端位移提交是如何存储的？具体工作机制是？
Kafka 消费位移提交到服务端存储在 __consumer_offset 的内部topic中, 位移以<GroupId, Topic, PartitionId> 作为key
以实际消费位移大小作为value(还包括其它信息)，存储在 __consumer_offset topic中的某一个分区中。当消费组内消费者出现
上线或者下线或者消费者重启时会从该内部topic下通过 key 获取 GroupId, Topic, PartitionId 指定的位移地址，然后再从log中找出该消息进行消费。



5. Kafka 消息的存储格式?
kafka消息存储分V0,V1和V2版本 三个不同的日志存储格式，Kafka版本对应日志格式版本的关系如下:
Kafka 0.10 以前: V0 版本日志
Kafka 0.10-0.11: V1 版本日志
Kafka 0.11 以后: V2 版本日志

kafka 消息按照 topic 进行划分，每个topic 下以分区进行细分，每个分区独立保存各自的数据(使用独立的offset)
分区为了保证高可用一般都会创建多个副本，多个副本按照一个leader 多个follower进行分工，leader直接对接生产者和
消费者，follower从leader同步数据做好数据的备份。 不论leader还是follower 都会将数据持久化到磁盘log文件，
实际log 文件在物理形式上一个文件夹，下面分多个segment,每个segment主要包含存储消息的.log 文件，.index稀疏索引
文件 和 .timeindex 时间索引文件。 这些文件以当前分区消息存储的offset 作为文件名。
其中log文件中存储的完整消息在 v2 版本中是以 batch 形式存储的。这里不展开具体格式，只做大致描述
(1) 生产者发送的ProducerBatch 对应在broker端为 RecordBatch ,批量发送过来的消息存储上也是批量存储
(2) 实际存储时每批消息数据在拉取时会被一次性拉取给到消费者,由消费者客户端进行批次内操作
(3) 压缩消息也是直接存储压缩后的消息,实际消息的解压缩处理交由消费者客户端完成



6. Kafka集群中Controller负责哪些工作?

7. Kafka 服务端幂等和事务指的是什么？如何使用?

8. Kafka 生产者客户端消息的发送流程?




################################################ 二 MySQL (熟练掌握) ###########################################
1. 聊一聊 MySQL的索引？
什么是索引? 索引的数据结构是怎样的? 为什么这样设计索引? 索引有哪些类型? 怎么应用索引?


2. 说一说MySQL的事务？
什么是事务? 有哪些特性? 特性是怎么实现的？事务如何做到隔离? RR隔离级别是如何实现的？


3. MySQL中有哪些锁?他们有什么用途?


4. MySQL中有哪些日志? 它们分别有什么用途?

5. 比较一下存储引擎 InnoDB 和 MyISAM？



############################################# 三 Redis (熟练原理&灵活应用) #########################################
1. Redis 有哪些数据结构? 它们底层是如何实现的？(Hash,Zset)

2. Redis 是单线程的为什么还这么快?

3. Redis 如何保证数据不丢失? 若是集群模式下如何保证?
I、非高可用情况下保证: 数据持久化(牺牲可用性保证可靠性)
Redis数据持久化有哪些方案? 两种持久化方案的工作机制? 数据如何刷盘?

II、高可用情况下保证: 集群节点故障后如何处理保证数据不丢失


4. 介绍一下Redis 集群?
Redis有哪些工作模式? 集群模式与主从模式的关系? 集群槽位划分(节点上下线)? 主从之间数据同步方式?





################################################ 四 分布式组件 (事务和锁) ###########################################


################################################ 五 JDK (并发&JVM&集合) ###########################################

1. JVM 如何有如何判断对象是否存活？有哪些垃圾收集算法？对应的又是哪些垃圾收集器?
JVM 判断对象是否存活主要 有引用计数法 和 可达性分析法 两种方法。
由于引用计数法存在循环引用的问题，因此JVM 虚拟机hotspot 采用的是可达性分析法。
可达性分析法，简单而言就是 通过设定一些GC Root, 从这些Root 出发遍历所有的树形引用
若对象没有出现在引用树上，就认为是可回收对象。常见的GC Root 有
a) 方法栈中的引用型变量
b）类静态属性及常量
c) 本地方法栈中的应用型变量
d) 虚拟机内部Class对象
e) 被同步锁持有的对象

JVM对垃圾收集采用分代收集的方式，用到的垃圾收集算法有
(1) 标记-清除法: 标记清除法采用先标记垃圾，然后将垃圾清除掉的方式回收
    它的问题在于标记阶段可能需要标记大量的垃圾，这样标记过程可能会存在性能问题，从而导致标记时性能不稳定
    另外再清除阶段，将垃圾清除后造成了堆区空间的不连续
(2) 标记-复制法:将内存划分区域,一部分使用，另一部分空闲，每次在使用的区域内将存活对象标记下来，然后将它们复制到空闲区域
    标记复制法在当存在大量朝生夕灭的对象时，可以只复制一小部分，并且由于使用复制的方式，回收垃圾的同时
    还保证了回收后区域的连续性。
    这种回收算法的问题在于，如果对象大多数都是存活的，需要复制的对象就会特别多，会存在严重的性能问题。
(3) 标记-整理法: 标记整理法也是先标记，然后将对象这里到内存区域的连续区域，整理完之后可回收对象被清理同时也没有内存区域的间断。
    标记整理法同样也存在性能问题,如需要回收区域的对象大多数都是存活的，在整理时也需要移动大量的存活对象，相较于标记复制的好处是
    不需要预留一部分空闲的内存空间。

常用的垃圾收集器，及搭配使用组合
标记-复制:
    标记复制算法主要用于新生代的垃圾回收,包括 Serial, ParNew,Parallel Scavenge 收集器都是使用的复制算法
    其中 Serial 是单线程进行回收, ParNew 是使用多线程进行回收 而 Parallel Scavenge 则是一款专注于吞吐量的垃圾收集器，
    譬如对于计算密集型的应用，更多的是希望譬如100s 内更多的时间花在计算上。前面的 Serial,ParNew 都是专注于停顿时间，则是IO密集型需要的
    同时 Parallel Scavenge区别于其它新生代垃圾收集器的一个重要特征是 它提供了自适应的细节参数，这就省去了研发人员的参数配置过程

标记-清除:
    CMS垃圾收集器是 Concurrent Mark Sweep 并发标记清除的首字母简称，也是经典垃圾收集器中唯一一个使用标记清除算法的收集器。
    它用于老年代的垃圾收集。CMS也是一款跨时代的垃圾收集器，区别之处在于它不需要stop the world 而是可以和用户线程一起运行并发的
    执行垃圾收集(并发标记&并发清除)。
    CMS垃圾收集器存在以下两个主要问题
    (a) 会存在浮动垃圾: 垃圾标记和清除过程都是并发完成的，在清除过程中是会出现新的垃圾，这就要求触发垃圾清除的老年代使用率不能
    是100% 而要比这个小, 譬如70%, 剩下的30% 用于GC时存放新产生的老年代对象。
    (b) 会存在碎片问题: CMS 使用的是标记清除算法,标记清除算法由于清除后造成了堆区内存的不连续,因此多次GC之后会看到
    老年代明明还有空间，但是就是不能被使用，就是因为大对象需要的连续空间不够。CMS提供了 CMSFullGCsBeforeCompaction参数来进行
    空间的整理，利用该参数可以设置每隔多少次Full GC 执行一次内存空间的压缩整理。

标记-整理:
    标记整理算法一般用在老年代的垃圾收集上，使用该算的垃圾收集器有 Serial Old 和 Parallel Old.
    Serial Old 是 Serial 的老年代版本。而 Parallel Old 则是 Parallel Scavenge 的老年代版本。



2. 线上出现OOM问题，该如何进行问题定位?



################################################ 六 系统设计(项目&临时设计) #########################################


############################################## 七 其它 (网络,系统,Maven,Git) ######################################