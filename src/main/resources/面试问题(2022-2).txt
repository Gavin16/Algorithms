################################################ 一 Kafka (精通) ################################################

1. Kafka 批量发送消息是如何存储的？批量消息是否需要批量消费? 压缩消息呢？
Kafka 批量发送或者压缩发送的消息都是按照发送过来的形式进行存储，并不做数据的解批或者解压缩处理.
由于存储就是按照批量进行的，消费者拉取到该批次消息进行消费时，会先将整批消息拉取下来存在消费者客户端
对应的容器中，然后根据消费者实际的消费方式(单条或者批量消费)进行ConsumerRecord的传递。在位移提交时
也是按照实际消费的数据进行提交，并不以存储的颗粒度进行提交。
假设topic-1 上面 1001 - 1100 这一百条消息是按照批进行存储的，那么实际拉取下来就是100条消息存在客户端缓存中，
若消费者是单条的方式进行消费，那么它每消费一条就进行一次offset 提交，而不是消费完所有100条再一次性进行offset提交。

压缩消息相对批量消息多了一个解压缩的过程，其它情况类似。


2. Kafka 有哪些情况会导致消息丢失?  哪些情况下消息会重复? 如何避免?

消息丢失:
①：消息生产者配置的acks = 0 或者 1,然后当某个分区leader节点故障后，如果此时消息还未被消费，并且follower也
	未同步到该消息（ISR中只有leader,没有其它follower）,新选举出来的follower就会因为从更小的offset开始记录，
	这时就导致了消息的丢失
②: 分区副本数为1, kafka默认日志刷盘是考操作系统取完成的，这样若某个分区的leader节点故障，同时消息未被刷盘的话，
	消息就会因为掉电而丢失
③: 消费者在消费消息时选择的自动提交，而不是手动提交。这样消费逻辑的执行出现异常，就会出现消费位移已经提交，但是部分逻辑执行失败
	这样对于失败的那部分逻辑而言消息是丢失了的。其实只要把消费接口做成幂等的，并且使用手提位移提交的方式就可以避免消费端的消息丢失。
以上消息丢失只需要对响应的配置做出修改即可


消息重复:
①: 生产者重复发送了消息，对于kafka而言这些消息是不一样的，但是业务角度来看都是相同的数据。
②: 消费者进行批量消费，消费到一半时出现异常，选择的手动提交消费位移，当消费者重新批量消费时已消费过的部分属于重复消费。
消息重复的情况只需要做好消费接口的幂等性设计即可。



3. Kafka 如何实现消息的顺序消费?

kafka 分区间不保证消息的顺序性，若需要对多分区进行消费同时还需要保证消息的顺序，是需要将消息拉取
下来做排序处理，之后再执行消费动作。该做法一方面效率较低，实现复杂，还面临消费位移提交的麻烦。
若条件允许可以通过以下两种消费方式实现顺序消费:
① 需要顺序消费的消息由单个生产者发送:
	单生产者发送的所有消息，在发送时就是按照顺序进行的，这时broker端指定分区leader在存储消息时也是顺序存储
    这时对应的消费者在消费时也是顺序的。这就是顺序消费了，这里关键的环节就是让这个生产者发送的消息都存储到
    一个分区中，通过相同的key 就能做到这种效果。

② 需要顺序消费的消息由多个生产者发送:
	多个生产者发送指定相同的key也可以将消息发送到同一个分区，但是多个生产者发送时消息的发送顺序和消息的存储顺序
	并不能做到一致，也就是说即使消息存储在了同一个分区，这里面的消息也是不保证有序的。
	这时就需要确保业务上的有序，可以考在消息体内增加序列号字段，选择足够大的消费批次，然后对不同key分区，对每组消息
	检查序列号是否完备，完备的进行消费，并计算出可提交的最大位移(没有间断)进行提交，同时做好消息的去重处理。




4. Kafka 消费端位移提交是如何存储的？具体工作机制是？
Kafka 消费位移提交到服务端存储在 __consumer_offset 的内部topic中, 位移以<GroupId, Topic, PartitionId> 作为key
以实际消费位移大小作为value(还包括其它信息)，存储在 __consumer_offset topic中的某一个分区中。当消费组内消费者出现
上线或者下线或者消费者重启时会从该内部topic下通过 key 获取 GroupId, Topic, PartitionId 指定的位移地址，然后再从log中找出该消息进行消费。



5. Kafka 消息的存储格式?
kafka消息存储分V0,V1和V2版本 三个不同的日志存储格式，Kafka版本对应日志格式版本的关系如下:
Kafka 0.10 以前: V0 版本日志
Kafka 0.10-0.11: V1 版本日志
Kafka 0.11 以后: V2 版本日志

kafka 消息按照 topic 进行划分，每个topic 下以分区进行细分，每个分区独立保存各自的数据(使用独立的offset)
分区为了保证高可用一般都会创建多个副本，多个副本按照一个leader 多个follower进行分工，leader直接对接生产者和
消费者，follower从leader同步数据做好数据的备份。 不论leader还是follower 都会将数据持久化到磁盘log文件，
实际log 文件在物理形式上一个文件夹，下面分多个segment,每个segment主要包含存储消息的.log 文件，.index稀疏索引
文件 和 .timeindex 时间索引文件。 这些文件以当前分区消息存储的offset 作为文件名。
其中log文件中存储的完整消息在 v2 版本中是以 batch 形式存储的。这里不展开具体格式，只做大致描述
(1) 生产者发送的ProducerBatch 对应在broker端为 RecordBatch ,批量发送过来的消息存储上也是批量存储
(2) 实际存储时每批消息数据在拉取时会被一次性拉取给到消费者,由消费者客户端进行批次内操作
(3) 压缩消息也是直接存储压缩后的消息,实际消息的解压缩处理交由消费者客户端完成



6. Kafka集群中Controller负责哪些工作?

7. Kafka 服务端幂等和事务指的是什么？如何使用?

8. Kafka 生产者客户端消息的发送流程?




################################################ 二 MySQL (熟练掌握) ###########################################
1. 聊一聊 MySQL的索引？
什么是索引? 索引的数据结构是怎样的? 为什么这样设计索引? 索引有哪些类型? 怎么应用索引?


2. 说一说MySQL的事务？
什么是事务? 有哪些特性? 特性是怎么实现的？事务如何做到隔离? RR隔离级别是如何实现的？


3. MySQL中有哪些锁?他们有什么用途?


4. MySQL中有哪些日志? 它们分别有什么用途?


5. 比较一下存储引擎 InnoDB 和 MyISAM？



############################################# 三 Redis (熟练原理&灵活应用) #########################################
1. Redis 有哪些数据结构? 它们底层是如何实现的？(Hash,Zset)

2. Redis 是单线程的为什么还这么快?

3. Redis 如何保证数据不丢失? 若是集群模式下如何保证?
I、非高可用情况下保证: 数据持久化(牺牲可用性保证可靠性)
Redis数据持久化有哪些方案? 两种持久化方案的工作机制? 数据如何刷盘?

II、高可用情况下保证: 集群节点故障后如何处理保证数据不丢失


4. 介绍一下Redis 集群?
Redis有哪些工作模式? 集群模式与主从模式的关系? 集群槽位划分(节点上下线)? 主从之间数据同步方式?
为什么Redis集群数据同步没有采用Kafka集群的acks=all的方式?




################################################ 四 分布式组件 (事务和锁) ###########################################
1. 分布式锁用redis 或 zookeeper分别怎么实现?


2. 分布式事务两阶段提交？TCC实现分布式事务的原理? Seata分布式事务的原理及注意事项?



################################################ 五 JDK (并发&JVM&集合) ###########################################

1. JVM 如何有如何判断对象是否存活？有哪些垃圾收集算法？对应的又是哪些垃圾收集器?
JVM 判断对象是否存活主要 有引用计数法 和 可达性分析法 两种方法。
由于引用计数法存在循环引用的问题，因此JVM 虚拟机hotspot 采用的是可达性分析法。
可达性分析法，简单而言就是 通过设定一些GC Root, 从这些Root 出发遍历所有的树形引用
若对象没有出现在引用树上，就认为是可回收对象。常见的GC Root 有
a) 方法栈中的引用型变量
b）类静态属性及常量
c) 本地方法栈中的应用型变量
d) 虚拟机内部Class对象
e) 被同步锁持有的对象

JVM对垃圾收集采用分代收集的方式，用到的垃圾收集算法有
(1) 标记-清除法: 标记清除法采用先标记垃圾，然后将垃圾清除掉的方式回收
    它的问题在于标记阶段可能需要标记大量的垃圾，这样标记过程可能会存在性能问题，从而导致标记时性能不稳定
    另外再清除阶段，将垃圾清除后造成了堆区空间的不连续
(2) 标记-复制法:将内存划分区域,一部分使用，另一部分空闲，每次在使用的区域内将存活对象标记下来，然后将它们复制到空闲区域
    标记复制法在当存在大量朝生夕灭的对象时，可以只复制一小部分，并且由于使用复制的方式，回收垃圾的同时
    还保证了回收后区域的连续性。
    这种回收算法的问题在于，如果对象大多数都是存活的，需要复制的对象就会特别多，会存在严重的性能问题。
(3) 标记-整理法: 标记整理法也是先标记，然后将对象这里到内存区域的连续区域，整理完之后可回收对象被清理同时也没有内存区域的间断。
    标记整理法同样也存在性能问题,如需要回收区域的对象大多数都是存活的，在整理时也需要移动大量的存活对象，相较于标记复制的好处是
    不需要预留一部分空闲的内存空间。

常用的垃圾收集器，及搭配使用组合
标记-复制:
    标记复制算法主要用于新生代的垃圾回收,包括 Serial, ParNew,Parallel Scavenge 收集器都是使用的复制算法
    其中 Serial 是单线程进行回收, ParNew 是使用多线程进行回收 而 Parallel Scavenge 则是一款专注于吞吐量的垃圾收集器，
    譬如对于计算密集型的应用，更多的是希望譬如100s 内更多的时间花在计算上。前面的 Serial,ParNew 都是专注于停顿时间，则是IO密集型需要的
    同时 Parallel Scavenge区别于其它新生代垃圾收集器的一个重要特征是 它提供了自适应的细节参数，这就省去了研发人员的参数配置过程

标记-清除:
    CMS垃圾收集器是 Concurrent Mark Sweep 并发标记清除的首字母简称，也是经典垃圾收集器中唯一一个使用标记清除算法的收集器。
    它用于老年代的垃圾收集。CMS也是一款跨时代的垃圾收集器，区别之处在于它不需要stop the world 而是可以和用户线程一起运行并发的
    执行垃圾收集(并发标记&并发清除)。
    CMS垃圾收集器存在以下两个主要问题
    (a) 会存在浮动垃圾: 垃圾标记和清除过程都是并发完成的，在清除过程中是会出现新的垃圾，这就要求触发垃圾清除的老年代使用率不能
    是100% 而要比这个小, 譬如70%, 剩下的30% 用于GC时存放新产生的老年代对象。
    (b) 会存在碎片问题: CMS 使用的是标记清除算法,标记清除算法由于清除后造成了堆区内存的不连续,因此多次GC之后会看到
    老年代明明还有空间，但是就是不能被使用，就是因为大对象需要的连续空间不够。CMS提供了 CMSFullGCsBeforeCompaction参数来进行
    空间的整理，利用该参数可以设置每隔多少次Full GC 执行一次内存空间的压缩整理。

标记-整理:
    标记整理算法一般用在老年代的垃圾收集上，使用该算的垃圾收集器有 Serial Old 和 Parallel Old.
    Serial Old 是 Serial 的老年代版本。而 Parallel Old 则是 Parallel Scavenge 的老年代版本。



2. 线上出现OOM问题，该如何进行问题定位?
OOM内存溢出按照不同维度划分可能有不同的情况 按照OOM报错内存分类可能有 栈区内存不够,堆区内存溢出,元空间内存溢出
按照业务的角度可能有: 代码中对大块数据进行处理(如查询不分页),连接服务端不释放连接导致连接占用大量内存等

下面主要从内存分类的角度来分析内存溢出的可能情况并给出一般化的定位方式
(1)栈区内存不够:
报错-> java.lang.OutOfMemoryError: java.lang.StackOverflowError
栈区内存溢出，跟栈空间总大小及每个线程栈最的空间大小有关；栈的总空间大小跟系统有关，不可设置。但是每个线程栈帧的大小
可以通过参数 -Xss 设置，-Xss设置的越大可以启动线程数就约多。
此外 如果某个线程内使用递归调用，递归嵌套数量太大也会导致 StackOverflowError。
因此 总结来看可以通过 -Xss参数设置 和 递归代码改造来解决栈区内存不够的问题。

定位方式:
总线程数太多导致栈空间不够分配给线程:
线程栈总可用内存 = JVM进程占用内存 -（-Xmx的值）- （-XX:MaxPermSize的值）- 程序计数器占用的内存
通过 jinfo 指令可以查看到 VM的参数配置，利用 top pid 可以查看到当前有多少个线程，然后通过上述公式
可以计算出总栈空间大小SS, SS - 线程数 * Xss 配置的栈大小 得到的结果是否特别小或者接近0, 如果是这样说明栈的总空间偏小
或者需要起更多节点来应对并发。

方法递归调用嵌套太多导致线程栈溢出:
异常栈信息中存在大量同名嵌套，这种情况就属于当个线程栈的栈空间溢出。

(2)堆区内存溢出:
报错-> java.lang.OutOfMemoryError:Java heap space
堆内存溢出可能的情况有:内存分配空间太小,内存泄露等
> 对于内存分配空间大小问题,刻意通过jinfo查看, 修改配置后可以解决

> 而对于内存泄露可以通过jmap -histo:live pid | more 指令查看按各类对象大小倒序排序的
  可以看到哪些对象占用的对空间明显较多，若对象是自定义的类对象可以再代码中review 找出问题代码。
  若非自定义代码可以


(3)元空间内存溢出:
报错-> java.lang.OutOfMemoryError: PermGen space
元空间一般不参与垃圾回收, 这部分内存溢出需要考虑是否是代码中使用了大量的反射等处理


3. 类加载过程?什么是双亲委派制?为什么使用双亲委派?





################################################ 六 系统设计(项目&临时设计) #########################################
1. 并发系统设计? 发红包/活动秒杀 等。
   活动商品秒杀:
   (1) 使用Redis 存储商品信息,redis先扣减，扣减成功后使用MQ异步保存用户-订单-商品信息
   (2) 使用 watch 乐观锁机制配合管道+事务完成批量指令的发送和执行
   (3) 当watch 的k-v 发生变更, 管道指令在执行时会返回NULL,若执行失败,返回的结果列表为空
       通过对 pipeline.execute() 的放回结果进行判断，可知当前执行执行有没有获取到锁，是否执行成功
   (4) MQ消费端消费生成 用户-订单-商品 的抢购记录(如何确保消息不丢失?)
   (5) 用户抢购之后抢断保存状态,首次查询未获取到数据提示重试,首次查询到之后缓存订单数据,之后每次查看不再走后端请求


   分抢十亿红包(金额随机,不能重复领取):


2. 功能系统设计? 如网站上看到的评论,弹幕,访问记录,分类排序等。

3.

############################################## 七 其它 (网络,系统,Maven,Git) ######################################
1. 系统进程间通信有哪些方式?

2. Nginx 实现网关功能参数?

