redis
客户端协议
线程模型
hash 实现的数据结构
redis hash的用法是 key field value 将field-value 整体来看记作val  key-val 本身就是一个hash的数据结构，它在实现时
使用的是数组 + 链表方式，然后field-value 在数据规模较小时使用压缩链表ziplist存储，当数据规模较大时，使用hashtable 存储
这样相当于在hashtable 中又嵌套了一层hashtable。类似于java 中hashtable实现 get/set 方法操作原理。。
不同的地方在于: 扩容rehash处理过程

对于rehash处理，redis 采用渐进是的rehash 处理，表现为当往 hashtable中添加元素时，若达到了判定的扩容条件，则会使用另一个新的
数组来存储数据，然后通过定时任务和每次的读/写/修改 操作来将源数组中的元素移动到新数组中。这样直到判断数组中所有元素都一到新数组，就将新数组的引用
赋值给原数组。

zset 实现数据结构
zset 使用方式为 zopt key member value

数据持久化
redis为内存数据持久化提供了两种数据类型，三种方案
RDB 文件作为内存快照数据，在redis重启加载时可以快速的将磁盘数据读入内存。
AOF 文件记录所有redis 的修改指令操作，每次修改操作之后可以根据不同的策略决定是否写文件，可以很好的避免数据丢失

三种持久化方案: RDB,AOF, 混合数据 RDB + AOF

RDB -> 对性能要求高，数据恢复要求不高的场景可以使用 RDB持久化方案，一般用在缓存持久化中
AOF -> 对性能要求不高，但是不希望有数据丢失的情况下，可以使用AOF持久化方案
RDB+AOF -> 混合持久化方案

集群工作原理
  主从同步
  sentinel 模式

hash set操作是否会更新过期时间 -- 不会更新有效时间，但是会更新LRU缓存淘汰时间,队列会变长
## 本地启动redis 服务,通过redis client访问调试

mysql
索引的工作原理: B-树 和 B+树
索引的分类: 主键索引，唯一索引，普通索引；聚簇索引，非聚簇索引；联合索引
执行计划表访问方法: const > ref > ref_or_null > range > index > ALL

const-> 主键或者 唯一二级索引等值查询
ref  -> 普通二级索引等值查询
ref_or_null -> ref包含值为null的情况
range -> 二级索引在某个范围内
index -> 扫描全部二级索引的查询方式(如 查询内容就在二级索引内,但是无法使用索引, 需要全表扫描)
ALL -> 全表扫描聚簇索引


索引下推
 -- 查询时重复利用索引中包含的条件，在向mysql服务器返回数据时将where条件做了过滤处理之后再返回给mysql服务端，以此减少mysql 服务端数据量
什么是快照读,什么是当前读
什么是事务,事务的特性
事务的隔离级别


RR级别事务实现原理 -- RR通过MVCC实现，简单来讲就是使用 readview + undolog + 数据隐藏字段(trx_id, roll_ptr) 来实现，RR隔离级别只在开启事务时创建对应的readview ，之后每次读取数据都是那同一个readview进行判断，判断方式为:
(1) 查询到数据后，判断当前记录的最后修改事务id 是否为当前事务ID,若是则判断为可见
(2) 记录的最后修改ID是否小于 readview 中up_trx_id ,小于的话意味着当前记录是已经提交过的事务，判断为可见
(3) 记录的最后修改ID是否大于 readview 中low_trx_id, 若大于则意味着开启readview 时，修改该记录的事务还没提交(没创建)，判断为不可见
(4) 若记录最后的修改ID  trx_id 满足 trx_id >= up_trx_id 且 trx_id <= low_trx_id 这时意味着修改当前记录的ID 在范围上没有问题
      就看它是否在创建readview 时已经提交了，如果提交了就可见，没提交的话就不可见；这时只需要判断 trx_id 是否在readview 中的 trx_ids 中即可
      若在trx_ids 的集合中，则判断创建readview时修改该记录的事务还活跃着--不可见。否则可见



锁的类型: 表锁/行锁; 悲观锁/乐观锁; 记录锁/间隙锁
死锁原因，如何避免


kafka
1.消息队列中间件如何选型


2.kafka为什么这么快
  从数据发送到接收过程来分析它为什么快的原因
  (1) 生产者支持消息批量发送，发送前可以做消息压缩，发送压缩后消息减少了网络带宽，可以一定程度提升发送效率
  (2) kafka 服务端对同一topic 做partition分区，不同分区接收来自不同客户端发送的消息，让集群能同时有多个分区响应请求
  (3) kafka 服务端每个分区在接收到数据时，并不直接做数据保存，而是使用到系统提供的pagecache 来保存数据
      减少了磁盘IO 可以进一步提升消息的处理性能
  (4) 数据落盘存储使用顺序写磁盘的方式，顺序写减少了磁盘寻址时间提升率写入速度
  (5) 消费者消费消息时，用到零拷贝技术。 具体而言就是分区服务端接收到消费请求后利用了linux 提供的sendfile 系统函数
       将存储在系统空间内pagecache中的数据，直接传给NIC网卡进行数据发送，减少了从系统空间拷贝到用户空间，再从用户空间拷贝回
       系统空间的频繁拷贝过程，同时也将线程上下文切换减少到2次


3.kafka消息堆积如何处理
   当消息的生产速度高于消息消费速度时，就可能出现消息的堆积。
   若服务上线前压测来看 生产速度 <= 消费速度  说明消费端出现了消费问题导致了消费速度变慢。
   -- 如果kafka在这里就是用来做削峰填谷的用途, 积压不是很严重甚至可以不处理

   一般分析排查过程:
   (1) 消费线程中是否使用了锁,锁的等待过程可能导致长时间等待,最终拖慢消费进度 -- 尽量避免锁的使用
   (2) 出现了费时的数据库请求，譬如慢查询，数据库锁等 -- SQL优化 或者 数据库锁优化
   (3) 消费线程中是否调用了其它微服务，或者三方服务。远程调用由于网络拥塞可能影响消费速度 -- 减少第三方服务调用/增加分区数与消费者节点数(线程池消费)
   (4) 消费者速度确实较慢 -- 扩容 增加分区数，同时增加消费者数，保持分区与消费组数一致



4.kafka如何保证消息不丢失
  消息发送:
    消息发送时可以采用异步回调方式，确保消息是发送到了服务端，并且服务端返回了响应，因此可以确保消息生产者能将消息发送到服务端。
    消息生产者发送时指定 ack="-1" ,同时设置最小ISR数量 > 1, 这样消息到了服务端之后会先做副本同步，然后才会返回发送成功通知给到生产者。

  消息消费:
    消费者消费消息时，会读取高水位HW以下的消息进行消费,这样只有消息做了所有ISR的同步才会被消费到，即使出现了节点挂了的情况，对应受影响分区选出新的leader
    也不影响消息的offset。
    消费者消费消息时设置手动提交消费offset, 确保不因为业务逻辑执行失败而提前提交，造成数据丢失。

5.kafka如何实现顺序消费
  不自定义分区器partitioner的情况下，可以直接使用相同的key;
  若key不能做到相同，可以定义相同的前缀或者后缀，然后自定义分区器，在分区期中对相同前缀或者
  后缀指定相同分区器。这样当前topic的所有消息将都发送到同一分区中，kafka 默认对同一分区中的数据消费是顺序进行的
  这样就可以实现kafka的顺序消费了。


6.kafka集群工作原理



7.kafka与rocketmq的区别

8. Kafka 压缩性能比较,消费端是否需要解压缩？





网络
https协议工作原理
三次握手四次挥手过程
常见的网络攻击有哪些? 怎么预防？

DDoS 攻击
1. SYN泛洪攻击
2. ARP报文泛洪攻击
3. Slowloris(慢连接攻击)


预防方式:
泛洪攻击 --
ARP报文泛洪攻击 --
Slowloris --


常见的网络安全问题有哪些? 有哪些应对策略?



tomcat,redis 连接池是怎么配置的？






自我介绍调整





JDK基础--JUC
volatile 和 synchronized 关键字
volatile 是轻量级锁，它可以保证被修饰属性在多线程读写情况下属性对多线程的可见性并能避免指令的重排序，在JMM内存模型中，默认为每个线层访问主存中的变量
维护一个副本，数据的修改只会在副本上，若使用volatile关键字修饰变量之后，可以再变量修改之后，将数据局刷新到主存，然后读线程也会从主存中进行
数据读取。
对于指令的重排序, volatile关键字修饰的属性在代码中被编译成机器指令时，会满足happen-before 原则，该原则即使做重排序，在原有代码顺序下会保证
属性的写发生在指令的读之前。

synchronized 关键字可以用来修饰属性和方法，线程对被修饰属性或者方法的时会被加上锁，加锁访问涉及了线程的上下文切换，因此被称synchronized 被称为重量级锁
被加锁代码的访问是串行进行的，该部分代码可以同时满足   原子性，可见性和有序性 。




ThreadLocal 工作机制(强/软/弱/虚引用)
ThreadLocal 对单个全局变量为每一个线程提供一个副本的维护，ThreadLocal 的工作涉及到以下组成部分
(1) ThreadLocal 中定义了 ThreadLocalMap
(2) Thread中维护了一个 ThreadLocal.ThreadLocalMap 的变量

对于定义的ThreadLocal变量，不同线程对ThreadLocal 变量进行get/set操作时，实际是以ThreadLocal 对象作为key,存入到ThreadLocal.ThreadLocalMap
中进行维护，对于不懂的线程而言虽然面对的事同一个ThreadLocal 变量，但是因为使用的是不同的线程，因此数据存到了不同的ThreadLocalMap 中。
通过以上机制，ThreadLocal实现了在多线程环境下的多副本维护。

需要注意的是 ThreadLocal 在使用时完之后,需要做remove处理，否则会出现内存泄露的情况。



ThreadLocal 为什么会内存泄露？？

ThreadLocalMap 中存的K-V 是 ThreadLocal 对象 和 value
一方面 ThreadLocal 作用域有限，为了让线程能在没有被引用时,可以正常释放内存，存入map中的entry是ThreadLocal的弱引用。
另一方面 ThreadLocalMap 作为线程的属性，他的生命周期跟线程绑定，这样即使ThreadLocal 对象被GC了，ThreadLocalMap 中
的entry依然还是存在的，因为次数value与entry的引用关系是引用，导致出现了key为null, 但是 value非null的情况。这些value即不能放访问到
有不能被回收(除非线程结束执行)。



CAS 操作
compare and set 在对全局变量进行访问前先和预期版本进行比较，若和预期一致则进行对应set操作(要求是原子操作)，若和预期版本不一致，则不做操作并返回
  根据使用场景需要，进行适当次数的重试可以实现乐观锁的效果。




AQS 工作原理
AQS 直译叫做抽象队列同步器, 从名字上来看首先它是一个抽象类，作为java并发包的基础主键，AQS中主要维护了三部分内容
(1) state 变量
(2) 当前独占线程
(3) 线程队列


AQS并发线程可以对AQS中的state进行操作，譬如 state 计数+1, 如果此时AQS应用在一个独占的场景下，就需要记住操作state + 1 的线程，
这样下一次有线程来进行操作acquire 时，通过与记录的线程比较看是否相同，相同则允许操作，不相同则将该线程加入到一个等待队列中,让这个线程先排队。
当独占线程调用release 操作时，AQS中的state对应的减1，判断state 是否等于 0 ，若为0 则释放当前线程对state 资源的锁定，然后通知队列中其它线程
或获得对state 的访问。这样借助 独占线程变量，CAS操作，线程队列，可以实现了一个对state 同步访问控制的基础组件。

虽然访问的只是一个整型变量 state , 但是AQS的应用涉及到 ReentrantLock, CountDownLatch, Semaphore等同步工具。 开发人员也可以利用AQS自定义
自己需要的同步器。


ThreadExecutorPool


AtomicLong
通过CAS操作来执行对 volatile long 变量的修改，实现线程安全的 long 类型



ConcurrentHashMap





JDK基础--JVM
内存模型





类加载机制？加载的处理流程？
java 应用程序编译成class文件后，需要有类加载器加载到JVM中，并在之后进行链接,初始化等处理

类的加载:
类的加载需要使用类加载器进行 提供的类加载器有  bootstrap classloader, extention classloader, application classloader;
此外用户还可以自定义类加载器。

bootstrap classloader: 用于加载jdk 的核心类库，如 java,sun,javax 包名开头的class文件。
extention classloader: 用于加载扩展类库，一般为jdk的扩展类库
application classloader: 用于加载classpath路径下的class文件

以上几个类加载器在使用时通过 双亲委派 机制协同进行工作; 在类加载的分工上存在  bootstrap classloader->extension classloader->application classloader
的父子关系，虽然类的实现上没有这种关系。
流程上简单来讲就是，如果一个类加载器收到了类加载的请求，它会首先将该任务委派给自己的父加载器，由父加载器进行加载，以此层层向上委派直到 bootstrap classloader
若bootstrap classloader加载失败，则会再把加载任务交回给到子类，直到当初被分配任务的子加载器。若这时子加载器还是加载失败，则报错 ClassNotFuondException

为什么要使用双亲委派机制?
为了保障类安全。假设有用户自定义了与核心类库相同全路径名的类，若没有双亲委派机制，譬如用户自定义了java.lang.Object，那么应用程序将java.lang.Object 加载进JVM后将导致出现完全相同的两个  java.lang.Object 类，这将带来巨大的安全隐患。
但是如果使用双亲委派机制， 最终加载进JVM的只会是 rt.jar 中的Object 类型。

此外对于同级的自定义类加载器，即使 DefinedClassLoader1 和 DefinedClassLoader2 加载了同一个类class文件，在JVM中他们依然是不同的类型。
JVM判断(包括equals, instanceof等)类型是否相同需要通过 类加载器+类全路径名一起判断。



类的链接验证:



类的初始化:





垃圾回收算法/垃圾回收器



对象的创建方式有哪些?



对象组成部分?
java 对象包含对象markword, klass指针(对象类(对象)所在元数据区地址),属性值(若是数组对象,则还需要算上数组长度,数组元素内容也在对象中)
对于64位jvm 若java对象头指针不被压缩,则默认klass 指针大小为64bit,压缩后是32bit

对于前32bit的markword,在对象不同的加锁状态在有不同的含义，对于未加锁情况下: 对象markword 中包含了对象被GC的次数, 对象偏向锁标识，对象的hash值
以及锁类型标识(无锁,轻量级锁，重量级锁等)


内存溢出(OOM)问题排查






分布式
seata 分布式事务有哪些模式？AT模式工作原理？
seata分布式事务支持 TCC,AT,XC,SAGA 四种模式；其中AT模式是无业务侵入的。
下面以AT模式为列介绍一下，它的实现方式。

(1) seata实现分布式事务是通过  TC(事务协调器), TM(事务管理器) 和 RM(资源管理器) 三部分组成的
    其中TC在服务端，负责统一协调事务的提交/回滚,全局锁,全局事务ID生成等
    TM在客户端负载各应用在分布式事务的事务管理，当需要提交和回滚时需要经过TM来通知到TC
    RM在客户端负责本地事务提交或回滚工作, 当个全局事务各分支本地事务已经执行完成，TC会通知RM进行分支提交






两阶段提交和三阶段提交？

两阶段提交：
(1) 事务提交协调者向参与者发送提交询问，参与者判断是否可以执行，并执行相关操作，若参与者执行成功则向协调者反馈"可以提交"
(2) 协调者若收到所有参与者都是反馈可以提交，则向所有参与者发送"提交" 请求；否则发送"回滚"请求
(3) 所有参与者执行对应的动作，并在执行完成后向 协调者发送 ack 响应

三阶段提交:
三阶段提交在二阶段提交的基础上增加了预提交操作,并在协调者和参与者身上都设置了超时时间。具体流程如下：
(1) 协调者向参与者发送canCommit 请求，参与者判断是否符合提交条件，若可以提交则返回 yes. 此时参与者并未真正开始执行事务动作。
(2) 协调者收到所有参与者发送的都是yes, 那么就判断可以进行提交；这时协调者继续向参与者发送 preCommit 请求。参与者收到preCommit请求后执行事务动作，若执行成功则向协调者返回 ack 响应。
(3) 当协调者收到所有参与者返回的针对 preCommit 的 ack响应后，协调者判断若所有参与者都没有问题，就发送doCommit请求。参与者收到doCommit 请求后执行最后的事务提交。并在提交完后后向协调者发送ack . 协调者收到所有ack之后，才完成整个事务的提交。
(4) 若在canCommit, preCommit 或者 doCommit 阶段出现了超时，譬如 协调者发送preCommit 时，有参与者一直没有返回 ack , 那么超时时间后，参与者将进行回滚，同时协调者也会向其它参与者发送回滚请求。
若在doCommit阶段，协调者发生故障没有向参与者发送 doCommit, 那么参与者也将自行进行事务提交。


TCC
Try - Confirm - Cancel 基于两阶段提交的






redis/zookeeper实现分布式锁的原理?
redis分布式锁基于 set key value ex nx 指令实现，当且仅当某个指令不存在时，指令才能执行成功。
这就类似于加锁过程，当没有其它线程在访问时，当前线程才能获取到资源。
简单来讲redis 实现分布式锁大致的工作原理如下:
(1)使用 redis set nx 指令来实现加锁操作；为了防止加锁线程异常终止导致锁一直处于无法释放状态，还需要给k-v加上一个过期时间
(2) 引入过期时间后，时间的大小设置就会成为问题，如果过期时间设置过小，加锁线程执行超时将导致锁被提前释放掉；若时间设置过长
    则在加锁线程异常终止后无法快速释放锁。因此需要引入一个守护线程来定时判断加锁线程是否存活，然后在另一方面为执行超时的
    线程提供超时时长延续。

(3) 锁的释放时，需要让不同线程能判断是不是自己加的锁，因此引入UUID 作为唯一标识；当且仅当时自己生成的UUID才可以进行锁的释放；
  防止锁的不规范使用(先释放锁，再加锁，然后再释放)

(4) 加锁失败的线程，可以通过tryLock的方式继续自旋等到，也可以设置超时时长，防止资源竞争激烈时长时间阻塞。


zookeeper分布式锁的实现？
zookeeper分布式锁的实现利用了zookeeper 临时节点和顺序节点以及watch机制，具体来讲就是:
1) 临时节点不能重复创建，且创建好之后若客户端失去连接，则临时节点将自动删除
2）顺序节点创建时，严格按照顺序编号来创建
3) zk客户端可以监听某个节点的事件，若监听删除事件，节点被删除时客户端可以收到相应的通知


zookeeper 就是通过创建临时顺序节点来实现锁之间的互斥访问，然后通过顺序节点间，前面节点被删除来通知下一个节点对应客户端获取到锁。
实现过程如下
(1) 当多个线程进行加锁处理时，这些线程使用zk客户端将在zk server端创建多个节点临时顺序节点，这些节点是按顺序排列的
(2) 然后每个加锁线程创建好节点之后，在本地判断自己创建的那个节点序列号是不是最小的，若判断是最小的则视为加锁成功，否则就是加锁失败
(3) 对于加锁成功线程直接返回处理业务逻辑就好；而对于加锁失败线程，他们可能需要通过二分查找找到自己节点的前一个节点，然后监听它; 并在此后进入等待状态
(4) 这样整体就进入一个  仅一个线程加锁成功，其余所有线程阻塞等待的状态；当加锁线程释放锁之后，zk server端对应的临时顺序节点将被删除
     这时被删除的临时顺序节点的下一个节点对应的客户端将受到通知，收到通知后再次判断当前所有顺序节点中，自己的节点是否是最小的，若是最小的则加锁成功

按照这种规则依次类推，最终就是实现了公平访问的分布式锁。


redis 和 zookeeper 分布式锁的优缺点？
redis分布式锁:
优点: redis具有较高的并发支撑能力，因此性能会较好
缺点: redis分布式锁实现上由于引入过期时间，同时需要考虑死锁问题，因此在设计上比较复杂

zookeeper 分布式锁:
优点:zookeeper实现分布式锁相对redis来讲要简单的多，无需维护过期时间，并且能做到公平锁
缺点:zookeeper分布式锁的性能不够高，因为zk 满足的事CAP理论的CP  具有强一致性，因此频繁的创建和删除节点，在服务端对应需要承受巨大的网络开销






MAVEN

maven作用域scope的含义
maven dependecy依赖中scope指的当前jar的在项目中的一个可见范围,常见的取值有 compile, provided, runtime,test 等
compile: 默认作用范围，所有时候都可见
provided: 编译时可见,运行时不可见；对于像servlet-api 这样的包，在编译时需要引入进来，否则会报错，但是在运行时就不需要了，因为tomcat容器会提供
runtime: 编译时不需要,在运行时才引入进来; 有些依赖倒置的设计，代码中只对接口进行调用，接口的具体实现可以做成插拔式的，譬如jdbc驱动包
test: 编译和运行时都不需要，仅在进行测试时才使用到；譬如junit包



项目介绍
调用链路拓补图:

性能压测:
指标:
压测主要关注 TPS, 百分位响应时间 以及 CPU和内存的资源消耗这几个指标。

压测方式:
参数配置:  目标服务采用 4C8G * 1 ，网关服务 4C8G * 3 ,  Nginx 使用8C16G , 数据库单独起服务配置 12C48G ，提供源API接口的服务 4C8G * 2

测试方式: 使用jmeter 分500 - 1000 线程，每个线程调用 500次，每次以50线程数作为递增步长，一共有10批次调用，每个线程完成一次调用后紧接着发起下一次调用，做到在没有线程完成所有调用时，每时刻都有请求在执行。然后在每批调用中，统计调用的吞吐量，以及90，95，99百分位响应时间，通过普罗米修饰统计获取容器的CPU和内存使用率。


压测遇到问题:
jmeter 客户端出现大量的  "IP:Port  fail to response" 以及 "gateway timeout" 的错误。
Kafka消息堆积: 大量调用消费端记录调用日志速度缓慢。



压测优化方案:
(0) jmeter 客户端:
启用keepalive 减少http调用TCP层握手次数


(1) Nginx 优化：
jmeter 客户端出现大量 fail to response 报错，定位发现是Nginx 接收到请求后并未成功将请求转发出去。
涉及优化参数有如下几个:
Nginx 与客户端连接：
keepalive_timeout: Nginx 与 客户端发送完最后一个请求后等待该时长后才释放TCP连接。
keepalive_requests:  每个keep-alive 的连接上可以发送的最大http请求数，超过该值时TCP连接将被释放。

Nginx 与上游服务(网关)连接:
proxy_send_timeout: nginx连接上游服务的读取服务内容的读取超时时间，若超过该时长则发送超时。
proxy_connect_timeout: nginx连接上游服务的连接超时时间。


(2) 网关优化:
网关层面并未做实质性的优化，只是简单的简单的节点的配置和节点数量。然后对该接口配置了放行白名单，直接进行请求的透传转发。

(3) Tomcat优化:



(4) 应用内优化:
应用转发超时时间设置
对于微服务压测接口，设置feignclient  连接超时时间不超过50ms, 读取超时时间不超过100ms。这些超时时间设置是为了让连接快速失败，避免因为某几个异常的连接常见阻塞某次调用。当然改设置对于转发外网的超时时间分别为3000ms 和 5000ms.


(5) JVM参数优化:
应用采用的容器化部署，通过 prometheus 观测发现pod 最大内存只能达到4G,相应的最大线程数只能达到210个，考虑到实际转发调用仅需要使用到少量的栈空间.  因此在JVM参数配置时将栈大小从1M 减小到 512K , 同时设置堆空间和非堆空间大小为2G ( pod配置内存的1/4 ) , 最终同时支持的线程和数量提升10% 到 230个。此外还有部分空间预留给到 skywalking 以及容器自身需要。


(6) 压测执行链路时间:
压测中每个环节都可能需要耗时量较大的情况，因此需要统计出整体耗时量较大的请求时间消耗的位置。


JVM堆区参数配置，按运维的说法应该是跟随了docker 容器的内存配置(JDK1.8 中间某个版本之后可以读取到容器参数配置)，实际压测时并未超出总大小的70%

最终测试结果:




扩展
搭建https服务(web服务器使用nginx)通过https 访问微服务
文档数据库完成API管理系统改造、
梳理输出spark框架面试常见问题